# PDF-Image-Based-Retrieval-Augmented-Generation-RAG-System
This system converts PDF pages into images, extracts detailed explanations from these images using a large language model (LLM), and stores them for querying using retrieval-augmented generation (RAG). The project allows users to retrieve contextually relevant explanations from Chroma's vector store and generate answers using Ollama's LLM, based on the content of PDF pages converted into images.

## Features
- **PDF to Image Conversion:** Convert each page of a PDF into an image.
- **Image Explanation Generation:** Use Ollama's LLM to generate detailed textual explanations for the images.
- **Chroma Vector Store:** Store the generated explanations along with embeddings in Chroma for efficient retrieval.
- **RAG Framework:** Query the stored explanations with context and generate answers using Ollama.

## Folder Structure

```
/pdf_ss
│
├── pdf_ss.py   # Script for taking images from pdf
├── embedding_stock/       # Folder to store the generated Chroma database
    ├── embedding.py #for embedding explanations generated by Ollama into the Chroma database
│   ├── explain/                  # Folder to persist the vector store
│
└── query.py                      # Script to query the stored embeddings and generate responses
```

## Prerequisites

Make sure you have Python 3.x installed along with the required libraries. You can install the dependencies using:

```bash
pip install langchain ollama pymupdf langchain-community pillow sentence-transformers chromadb
```

## Usage

### Step 1: Convert PDF to Images and Generate Embeddings

Use the script `pdf_screenshot_embedding.py` to:
- Convert the pages of a PDF to images.
- Generate detailed explanations for each image.
- Store the explanations in a Chroma vector store.

#### Run `pdf_screenshot_embedding.py`:
```bash
python pdf_screenshot_embedding.py
```

This script will:
1. Convert each page of the provided PDF into images and save them in the specified output folder (`output_images`).
2. Use Ollama's LLM to generate explanations for each image.
3. Split the explanations into chunks and store them in the Chroma vector store located in `embedding_stock/explain`.

### Step 2: Querying the Vector Store

After the embeddings are stored, you can query the vector store using `query.py`. The script retrieves relevant explanations from Chroma and uses Ollama to generate context-aware responses.

#### Run `query.py`:
```bash
python query.py
```

The script will:
- Accept a query from the user.
- Retrieve relevant explanations from the Chroma vector store.
- Pass the retrieved context to Ollama to generate a detailed response.

### Code Walkthrough

#### `pdf_ss.py`

This script performs the following operations:
1. **Convert PDF pages to images:** Uses `PyMuPDF` to convert each page into a PNG image.

###`embedding.py`
1. **Generate image explanations:** For each image, the explanation is generated using Ollama’s LLM (`llava:7b`).
2. **Split and store embeddings:** Each explanation is split into smaller chunks using `CharacterTextSplitter`, and then stored in Chroma along with the embeddings.

#### `query.py`

This script handles user queries:
1. **Query input:** Prompts the user to enter a query.
2. **Retrieve relevant documents:** The script uses Chroma to retrieve relevant explanations based on the query.
3. **Generate context-aware response:** The retrieved context is fed into Ollama to generate a final response.

## Example Usage

### Step 1: Convert PDF to Images and Generate Embeddings

```bash
python pdf_screenshot_embedding.py
```

After running this command, images will be stored in `output_images`, and the explanations will be saved in the Chroma vector store in the `embedding_stock/explain` folder.

### Step 2: Querying the Vector Store

Run the query script to interact with the system and receive generated responses.

```bash
python query.py
```

Enter a query like: "What does the image explain about the stock market?"

The script will fetch relevant explanations, pass them to Ollama, and return the generated response.

## Troubleshooting

- **Missing dependencies:** Ensure you have installed all required packages using the `pip` command above.
- **Ollama issues:** Make sure you have access to Ollama's API and are using the correct model identifier.
- **Chroma persistence issues:** If you encounter issues with Chroma, ensure the `embedding_stock/explain` directory is writable and the Chroma vector store is correctly set up.

